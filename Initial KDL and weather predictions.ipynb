{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd  \n",
    "import datetime\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, kurtosis, skew #correlation coefficient, skewness, and kurtosis \n",
    "import seaborn as sns #used for color palletes on graphs and boxplots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "from statsmodels.tsa.stattools import adfuller # Dicky Fuller test for stationarity\n",
    "import statsmodels.api as smm \n",
    "import statsmodels.formula.api as sm  # Multiple Linear Regression\n",
    "from statsmodels.graphics.tsaplots import plot_acf #autocorrelation plot \n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX # SARIMA Model import\n",
    "from pylab import rcParams # Decomposition of time series\n",
    "\n",
    "#Another cool model I want to use but can't get to properly install\n",
    "#from fbprophet import prophet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Merging Visitor + Circulation Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in daily circulation data (old method)\n",
    "#circ_day = pd.read_excel(r\"data\\KDL Data.xlsx\", sheet_name = \"phys_circ_day\").sort_values(by = \"date\")\n",
    "#circ_day['date'] = pd.to_datetime(circ_day['date'], format='%Y-%m') # Converts the date variable into a datetime variable \n",
    "#circ_day = circ_day.drop(columns= [\"day\"]) #Dropping the day variable \n",
    "\n",
    "# Reading in new daily circulation data (covers from 2016-2023) \n",
    "circ_day = pd.read_excel(r\"data\\new_daily_checkouts.xlsx\").sort_values(by = \"date\")\n",
    "circ_day['date'] = pd.to_datetime(circ_day['date'], format='%Y-%m') # Converts the date variable into a datetime variable \n",
    "circ_day = circ_day.drop(columns= [\"open_hours\"]) #Dropping the open hours variable \n",
    "\n",
    "# Reading in daily visitor data \n",
    "visit_day =  pd.read_excel(r\"data\\KDL Data.xlsx\", sheet_name = \"daily_visits\").sort_values(by = \"date\")\n",
    "visit_day = visit_day[[\"date\", \"branch\" ,\"door_count\"]] #Selecting specific variables\n",
    "\n",
    "#Merging the visitor count and circulation data together by date and branch colums\n",
    "mergedWV = pd.merge(circ_day, visit_day, on= ['date', 'branch'], how='left')\n",
    "\n",
    "#Filtering KDL data to only include dates before July 12th to match weather data + filtering out useless branches \n",
    "#mergedWV = mergedWV[(mergedWV[\"date\"] >= \"8/2/2021\") & (mergedWV[\"date\"] <= \"6/24/2023\") & (~mergedWV[\"branch\"].isin([\"BKM\", \"GFL\", \"MELCAT\", \"SC\", \"GTN\"]))]\n",
    "\n",
    "mergedWV = mergedWV[(mergedWV[\"date\"] >= \"8/2/2021\") & (~mergedWV[\"branch\"].isin([\"BKM\", \"GFL\", \"MELCAT\", \"SC\", \"GTN\"]))]\n",
    "\n",
    "# Creating a df with only the dates to then fill in the missing dates in the specifc branch dfss \n",
    "start_date = datetime(2021, 8, 2)\n",
    "end_date = datetime(2023, 12, 13)\n",
    "#end_date = datetime(2023, 6, 24)\n",
    "date_range = pd.date_range(start=datetime(2021, 8, 2), end=end_date, freq='D')\n",
    "\n",
    "df_dates = pd.DataFrame({'date': date_range})\n",
    "\n",
    "\n",
    "#Setting the index to just be the date variable \n",
    "mergedWV = mergedWV.set_index('date')\n",
    "\n",
    "\n",
    "#list of branches I want to specifically look \n",
    "branches = [\"EGR\", \"GDV\", \"CAS\"] \n",
    "branch_dfs = {}\n",
    "\n",
    "def expand_dates(df):\n",
    "    \"\"\"\n",
    "    Create time series features based on time series index.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    #We use the map function along with a lambda function to apply the strftime method to each datetime object in the index. \n",
    "    # This approach is necessary because the strftime method is designed to work with individual datetime objects, not with entire Series or DataFrames\n",
    "\n",
    "    df['dayofweekchar'] = df.index.map(lambda x: x.strftime('%A'))  # Full day name, e.g., \"Monday\" \n",
    "    # Define the desired order of days of the week\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    # Convert 'day_of_week_char' to a categorical variable with the specified order\n",
    "    df['dayofweekchar'] = pd.Categorical(df['dayofweekchar'], categories=day_order, ordered=True)\n",
    "\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['month'] = df.index.month\n",
    "    df['month_char'] = df.index.map(lambda x: x.strftime('%b'))  # Full month name, e.g., \"September\"\n",
    "\n",
    "    # Define the custom order for the month names\n",
    "    custom_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    # Convert 'month_char' to a categorical data type with the custom order\n",
    "    df['month_char'] = pd.Categorical(df['month_char'], categories=custom_order, ordered=True)\n",
    "     # Sort the DataFrame based on the custom order\n",
    "\n",
    "    df['year'] = df.index.year\n",
    "\n",
    "    #turns it not to numeric for whatever reasosn\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    df['dayofyear'] = pd.to_numeric(df['dayofyear'])\n",
    "\n",
    "    df['dayofmonth'] = df.index.day\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    return df\n",
    "\n",
    "\n",
    "#More efficient to run this function once on the entire dataset then can just use the loop to filter it\n",
    "mergedWV = expand_dates(mergedWV)\n",
    "\n",
    "for branch in branches:\n",
    "\n",
    "    #Assigning each branch to an index in the branch_dfs dictionary\n",
    "    branch_dfs[branch] = mergedWV[mergedWV[\"branch\"] == branch]\n",
    "\n",
    "    branch_dfs[branch] =  branch_dfs[branch].drop(columns = [\"branch\"], axis=1)\n",
    "\n",
    "    branch_dfs[branch] = pd.merge(df_dates, branch_dfs[branch], on= ['date'], how='left') # Adds all missing dates to each branch df\n",
    "\n",
    "    branch_dfs[branch]['open'] = 1  # Open by default\n",
    "\n",
    "    branch_dfs[branch]['open'].loc[branch_dfs[branch]['door_count'].isna()] = 0  # Closed on Sundays in the summer\n",
    "\n",
    "    branch_dfs[branch] = branch_dfs[branch].set_index('date') #Setting the date variable as the index for all newly created dfs \n",
    "    \n",
    "    globals()[f\"{branch}_dataframe\"] = branch_dfs[branch] #The globals function allows you to access the variables in the current environment with the f string \n",
    "\n",
    "#Cleaning up the envirnment \n",
    "del branch, branch_dfs, branches\n",
    "\n",
    "#circ_day.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ways of handeling missing values in the branch dfs  \n",
    "\n",
    "#EGR_dataframe['transactions'].fillna(method='ffill', inplace=True) #Forward Fill method \n",
    "\n",
    "EGR_dataframe['transactions'].fillna(0, inplace=True) #Filling in missing values with 0\n",
    "\n",
    "#EGR_dataframe['transactions'].interpolate(method='linear', inplace=True) #linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "if EGR_dataframe.index.name == 'date':\n",
    "    print(\"The 'date' variable is properly saved as the index.\")\n",
    "else:\n",
    "    print(\"The 'date' variable is not the index or not named as 'date'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collinearity Checks Among the Predictors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation maxtrix among all the library varaibles \n",
    "\n",
    "correlation_matrix = EGR_dataframe.corr(numeric_only = True) \n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix between just on the variables I'm using to measure days/time\n",
    "\n",
    "correlation_matrix_time = EGR_dataframe[['dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'dayofmonth', 'weekofyear']].corr()\n",
    "sns.heatmap(correlation_matrix_time, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(EGR_dataframe[['transactions', 'door_count']])\n",
    "plt.title('Pair Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circulation EDA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EGR_dataframe[\"transactions\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot (Matplotlib) - Might be cool to add a distriubtion fit like the seaborn pack automatically does\n",
    "\n",
    "# Calculate kurtosis and skewness\n",
    "kurtosis_trans = kurtosis(EGR_dataframe[\"transactions\"])\n",
    "skewness_trans = skew(EGR_dataframe[\"transactions\"])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(EGR_dataframe[\"transactions\"], density=True, bins=30, alpha=0.5, color='blue')\n",
    "\n",
    "plt.xlabel('X-axis Label')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f'Density Plot\\nKurtosis: {kurtosis_trans:.2f}, Skewness: {skewness_trans:.2f}')\n",
    "\n",
    "\n",
    "plt.text(0.7, 0.9, f'Kurtosis: {kurtosis_trans:.2f}', transform=plt.gca().transAxes, fontsize=12) #The transform=plt.gca().transAxes argument specifies that the text coordinates are given relative to the axes.\n",
    "plt.text(0.7, 0.80, f'Skewness: {skewness_trans:.2f}', transform=plt.gca().transAxes, fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density plot (Seaborn)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.kdeplot(EGR_dataframe[\"transactions\"], fill=True, color='green')\n",
    "\n",
    "plt.xlabel('X-axis Label')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot using Seaborn')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ciruclation by day over time at EGR KDL just checking out the trends and seasonality \n",
    "\n",
    "# Plotting the line graph with modified figure size\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(EGR_dataframe.index, EGR_dataframe['transactions'])\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Transactions')\n",
    "plt.title('Daily Transactions Since 2021')\n",
    "plt.grid(True)\n",
    "\n",
    "# Format x-axis date labels\n",
    "date_formatter = mdates.DateFormatter(\"%b '%y\")\n",
    "plt.gca().xaxis.set_major_formatter(date_formatter)\n",
    "\n",
    "# Set a custom date locator to show every other month\n",
    "month_locator = mdates.MonthLocator(interval=2)\n",
    "plt.gca().xaxis.set_major_locator(month_locator)\n",
    "\n",
    "# Fit a linear regression line (line of best fit)\n",
    "x = np.arange(len(EGR_dataframe))\n",
    "y = EGR_dataframe['transactions']\n",
    "coefficients = np.polyfit(x, y, 4)  # fourth-degree polynomial\n",
    "trendline = np.poly1d(coefficients)\n",
    "\n",
    "# Plot the trendline\n",
    "plt.plot(EGR_dataframe.index, trendline(x), color='red', linestyle='--', label='Trendline')\n",
    "\n",
    "# Adding a vertical line where we split the dataset at the beginning of 2023\n",
    "plt.axvline(pd.to_datetime('01-01-2023'), color='black', linestyle='--', label='Vertical Line at x=50')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Circulation by day of week (0 is Monday and 6 is Sunday)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.boxplot(data=EGR_dataframe, x='dayofweekchar', y='transactions', showmeans = True)\n",
    "\n",
    "plt.xlabel(None)\n",
    "plt.ylabel('Daily Transactions')\n",
    "ax.set_title('Transactions by Day of Week')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circulation by month - also good indicator of seasonality\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.boxplot(data=EGR_dataframe, x='month_char', y='transactions', palette='Blues', showmeans = True)\n",
    "\n",
    "ax.set_title('Transactions by Month')\n",
    "plt.xlabel(None)\n",
    "plt.ylabel('Daily Transactions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circulation by quarter \n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.boxplot(data=EGR_dataframe, x='quarter', y='transactions', palette='Blues', showmeans = True)\n",
    "\n",
    "ax.set_title('Daily Transactions by Quarter')\n",
    "plt.xlabel(None)\n",
    "plt.ylabel('Transactions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plot\n",
    "\n",
    "plot_acf(EGR_dataframe['transactions'], lags=30)  # Adjust the number of lags as needed\n",
    "\n",
    "plt.title('Autocorrelation Plot for Transactions')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visitor EDA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EGR_dataframe[\"door_count\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot (Matplotlib)\n",
    "\n",
    "# Calculate kurtosis and skewness - CAN'T CALCULATE KURTOSIS AND SKEWNESS HERE BECAUSE OF MISSING VALUES \n",
    "kurtosis_visit = kurtosis(EGR_dataframe[\"door_count\"])\n",
    "skewness_visit = skew(EGR_dataframe[\"door_count\"])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(EGR_dataframe[\"door_count\"], density=True, bins=30, alpha=0.5, color='blue')\n",
    "plt.xlabel('visitor count')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f'Density Plot\\nKurtosis: {kurtosis_visit:.2f}, Skewness: {skewness_visit:.2f}')\n",
    "\n",
    "plt.text(0.7, 0.9, f'Kurtosis: {kurtosis_visit:.2f}', transform=plt.gca().transAxes, fontsize=12) #The transform=plt.gca().transAxes argument specifies that the text coordinates are given relative to the axes.\n",
    "plt.text(0.7, 0.80, f'Skewness: {skewness_visit:.2f}', transform=plt.gca().transAxes, fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the line graph with modified figure size\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(EGR_dataframe.index, EGR_dataframe['door_count'])\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Visitors')\n",
    "plt.title('Daily Visitors Since 2021')\n",
    "plt.grid(True)\n",
    "\n",
    "# Format x-axis date labels\n",
    "date_formatter = mdates.DateFormatter(\"%b '%y\")\n",
    "plt.gca().xaxis.set_major_formatter(date_formatter)\n",
    "\n",
    "# Set a custom date locator to show every other month\n",
    "month_locator = mdates.MonthLocator(interval=2)\n",
    "plt.gca().xaxis.set_major_locator(month_locator)\n",
    "\n",
    "# Fit a linear regression line (line of best fit)\n",
    "x = np.arange(len(EGR_dataframe))\n",
    "y = EGR_dataframe['door_count']\n",
    "coefficients = np.polyfit(x, y, 6)  # fourth-degree polynomial\n",
    "trendline = np.poly1d(coefficients)\n",
    "\n",
    "# Plot the trendline\n",
    "plt.plot(EGR_dataframe.index, trendline(x), color='red', linestyle='--', label='Trendline')\n",
    "\n",
    "# Adding a vertical line where we split the dataset at the beginning of 2023\n",
    "plt.axvline(pd.to_datetime('01-01-2023'), color='black', linestyle='--', label='Vertical Line at x=50')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visitors by day of week \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.boxplot(data=EGR_dataframe, x='dayofweekchar', y='door_count', showmeans = True)\n",
    "\n",
    "plt.xlabel(None)\n",
    "plt.ylabel(\"Visitors\")\n",
    "ax.set_title('Visitors by Day of Week')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visitors by month \n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.boxplot(data=EGR_dataframe, x='month_char', y='door_count', palette='Blues', showmeans = True)\n",
    "\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Visitors\")\n",
    "ax.set_title('Visitors by Month')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plot\n",
    "\n",
    "plot_acf(EGR_dataframe['door_count'], lags=30)  # Adjust the number of lags as needed\n",
    "\n",
    "plt.title('Autocorrelation Plot for Door Count')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Weather Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_excel(r\"data/weather data.xlsx\", sheet_name= \"weather\")\n",
    "\n",
    "#Creating the a date variable based on the MO, DY, and YEAR columns\n",
    "weather[\"date\"] =  weather[\"MO\"].astype(str) + \"/\" + weather[\"DY\"].astype(str) + \"/\" + weather[\"YEAR\"].astype(str) #concating only works with strings \n",
    "weather[\"date\"] = pd.to_datetime(weather[\"date\"], format = \"%m/%d/%Y\" ) # need to use the pandas to datetime function because the newly created date variable is a pandas object and not a string\n",
    "\n",
    "#Renaming all the columns to be more readable\n",
    "weather = weather.rename(columns = {\"MO\":\"Month\", \"DY\":\"Day\", \"YEAR\":\"Year\", \"T2M\":\"temperature\", \"T2MDEW\":\"dew_point\", \"T2M_RANGE\":\"temp_range\", \"T2M_MAX\":\"temp_max\", \"T2M_MIN\":\"temp_min\", \"RH2M\": \"relative_humidity\", \"PRECTOTCORR\": \"precipitation\", \n",
    "                                    \"PS\": \"pressure\", \"WS10M\": \"wind_speed\", \"WND10M_S\": \"wind_speed_max\", \"WS10M_MAX\": \"wind_max\", \"WS10M_MIN\": \"wind_min\", \"WS10M_RANGE\": \"wind_range\"}) \n",
    "\n",
    "#Drop specific variables from our data frame\n",
    "weather_describe = weather.drop(columns= [\"Year\", \"Month\", \"Day\", \"QV2M\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the second weather dataframe \n",
    "\n",
    "weather2 = pd.read_csv(r\"data/weather2.csv\")\n",
    "\n",
    "#Renaming all the columns to be more readable\n",
    "weather2 = weather2.rename(columns = {\"PRCP\":\"Precipitation\", \"TMAX\":\"max_temp\", \"TMIN\":\"min_temp\", \"DATE\":\"Date\"}) \n",
    "\n",
    "weather2[\"Date\"] = pd.to_datetime(weather2[\"Date\"], format = \"%m/%d/%Y\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics for Key Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for key variables \n",
    "\n",
    "# List of important weather variables to plot\n",
    "y_variables = [\"wind_speed\", \"temperature\", \"precipitation\", \"dew_point\", \"pressure\"]\n",
    "\n",
    "for variable in y_variables:\n",
    "    print(f'\\nSummary Statistics for {variable}:\\n{round(weather_describe.describe()[variable], 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of important weather variables to plot\n",
    "y_variables = [\"wind_speed\", \"temperature\", \"precipitation\", \"dew_point\", \"pressure\"]\n",
    "\n",
    "# Create subplots to display multiple boxplots\n",
    "fig, axes = plt.subplots(nrows=len(y_variables), ncols=1, figsize=(8, 5 * len(y_variables)))\n",
    "\n",
    "# Loop through each variable and create the boxplot\n",
    "for i, variable in enumerate(y_variables):\n",
    "    data_to_plot = weather_describe[variable]\n",
    "    axes[i].boxplot(data_to_plot, vert=True, showmeans= True)  # Use vert=False to create horizontal boxplots\n",
    "\n",
    "    axes[i].set_xlabel(None)\n",
    "    axes[i].set_ylabel(variable.title())\n",
    "    axes[i].set_title(f'Boxplot for {variable.title()}')\n",
    "\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Adjust the layout of subplots and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution for Key Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of important weather variables to plot\n",
    "y_variables = [\"wind_speed\", \"temperature\", \"precipitation\", \"dew_point\", \"pressure\"]\n",
    "\n",
    "# Create subplots to display multiple density plots\n",
    "fig, axes = plt.subplots(nrows=len(y_variables), figsize=(8, 5*len(y_variables)))\n",
    "\n",
    "# Loop through each y-variable and create the density plot\n",
    "for i, variable in enumerate(y_variables):\n",
    "    var_kurtosis = kurtosis(weather_describe[variable])\n",
    "    var_skewness = skew(weather_describe[variable])\n",
    "\n",
    "    axes[i].hist(weather_describe[variable], density=True, bins=30, alpha=0.5, color='blue')\n",
    "\n",
    "    axes[i].set_xlabel(f'{variable.title()}')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].set_title(f'Density Plot for {variable.title()} \\nKurtosis: {var_kurtosis:.2f}, Skewness: {var_skewness:.2f}')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "    axes[i].text(0.7, 0.9, f'Kurtosis: {var_kurtosis:.2f}', transform=axes[i].transAxes, fontsize=12)\n",
    "    axes[i].text(0.7, 0.80, f'Skewness: {var_skewness:.2f}', transform=axes[i].transAxes, fontsize=12)\n",
    "\n",
    "# Adjust the layout of subplots and show the plots\n",
    "#plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.5) # Adjusts the space between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Key Weather Varaibles Over Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Graphs \n",
    "\n",
    "# List of important weather variables to plot\n",
    "y_variables = [\"wind_speed\", \"temperature\", \"precipitation\", \"dew_point\", \"pressure\"]\n",
    "\n",
    "# Static X Variable \n",
    "x = weather[\"date\"]\n",
    "\n",
    "# Create separate subplots for each y-variable\n",
    "num_plots = len(y_variables)\n",
    "fig, axs = plt.subplots(num_plots, figsize=(10, 5 * num_plots)) #Subplots functions creates a seperate plot for each varaible (not stacking them on same graph)\n",
    "\n",
    "# Loop through each y-variable and plot it on a separate subplot\n",
    "for i, y_variable in enumerate(y_variables):\n",
    "    y = weather[y_variable]\n",
    "    axs[i].plot(x, y)\n",
    "    axs[i].set_xlabel(\"Date\")\n",
    "    axs[i].set_ylabel(y_variable.title())\n",
    "    axs[i].set_title(f\"{y_variable.title()} Over Time\")\n",
    "    axs[i].grid(True)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for key variables by month\n",
    "\n",
    "# List of important weather variables to plot\n",
    "y_variables = [\"wind_speed\", \"temperature\", \"precipitation\", \"dew_point\", \"pressure\"]\n",
    "\n",
    "# Choosing the X variable for how I want to group the data\n",
    "x = 'Month'   # Year Month Day\n",
    "\n",
    "# Create subplots to display multiple boxplots\n",
    "fig, axes = plt.subplots(nrows=len(y_variables), ncols=1, figsize=(10, 5 * len(y_variables)))\n",
    "\n",
    "# Loop through each variable and create the boxplot\n",
    "for i, variable in enumerate(y_variables):\n",
    "    sns.boxplot(data=weather, x=x, y=variable, palette='Blues', showmeans=True, ax=axes[i])\n",
    "\n",
    "    axes[i].set_xlabel(None)\n",
    "    axes[i].set_ylabel(variable.title())\n",
    "    axes[i].set_title(f'Boxplot for {variable.title()}')\n",
    "\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Adjust the layout of subplots and show the plots\n",
    "#plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megring the Weather and Circulation Datasets Together Based on Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EGR_all = pd.merge(EGR_dataframe, weather, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EGR_all = EGR_all.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate anaylsis: Weather Data vs. Circulation/Visitor Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already defined EGR_all and x here\n",
    "\n",
    "# Define X variable \n",
    "x = \"transactions\" # door_count transactions\n",
    "x_var = EGR_all[\"transactions\"] \n",
    "\n",
    "# List of key weather variables to iterate through against our x variable\n",
    "y_variables = [\"wind_speed\", \"temperature\", \"precipitation\", \"dew_point\", \"pressure\", 'door_count']\n",
    "\n",
    "# Create subplots to display multiple plots\n",
    "fig, axes = plt.subplots(nrows=len(y_variables), ncols=1, figsize=(8, 5*len(y_variables)))\n",
    "\n",
    "# Loop through each y variable and create the scatter plot\n",
    "for i, variable in enumerate(y_variables):\n",
    "    y = EGR_all[variable]\n",
    "\n",
    "    # Calculate the line of best fit using numpy's polyfit function\n",
    "    coefficients = np.polyfit(x_var, y, 1)\n",
    "    polynomial = np.poly1d(coefficients)\n",
    "\n",
    "    # Calculate Pearson's correlation coefficient (R) and its p-value\n",
    "    correlation_coefficient, p_value = pearsonr(x_var, y)\n",
    "\n",
    "    # Create the scatter plot\n",
    "    axes[i].scatter(x_var, y, label='Data Points')\n",
    "\n",
    "    # Add the line of best fit to the plot\n",
    "    axes[i].plot(x_var, polynomial(x_var), color='red', label='Line of Best Fit')\n",
    "\n",
    "    # Add the correlation coefficient to the plot\n",
    "    axes[i].text(1375, np.mean(y), f\"Pearson's R: {correlation_coefficient:.2f}\", fontsize=12)\n",
    "\n",
    "    # Add labels and legend\n",
    "    axes[i].set_xlabel(f'{x.title()}')\n",
    "    axes[i].set_ylabel(variable.title())\n",
    "    axes[i].set_title(f'Scatter Plot Between {x.title()} and {variable.title()}')\n",
    "    axes[i].grid(True)\n",
    "    axes[i].legend()\n",
    "\n",
    "# Adjust the layout of subplots and show the plots\n",
    "plt.tight_layout() # Adjusts the spacing between subplots to make sure they don't overlap \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### I could make this model even better potentially by adding in data for all the missing value days ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into a training and testing set\n",
    "trainMLR = EGR_all.loc[EGR_all.index < '01-01-2023']\n",
    "testMLR = EGR_all.loc[EGR_all.index >= '01-01-2023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "EGR_all.columns\n",
    "\n",
    "# Create a linear regression model\n",
    "MLR_model = sm.ols('transactions ~ door_count + dayofweek + quarter + month + year + dayofyear + dayofmonth + dayofyear + precipitation + temp_max  + relative_humidity' , data=trainMLR).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(MLR_model.summary())\n",
    "\n",
    "############ Potentially try to insitutute forwards, backward, or stepwise elimination??? ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption Checking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form of Model Assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values and residuals\n",
    "predicted_values = MLR_model.predict()\n",
    "residuals = MLR_model.resid\n",
    "\n",
    "#Residual vs predicted plot \n",
    "# Checking Linearity, mean zero, constant variance\n",
    "\n",
    "plt.scatter(predicted_values, residuals)\n",
    "\n",
    "plt.title('Residuals vs. Predicted Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual vs Fitted \n",
    "\n",
    "# fitted values\n",
    "model_fitted_y = MLR_model.fittedvalues\n",
    "\n",
    "#  Plot\n",
    "plot = sns.residplot(x=model_fitted_y, y = predicted_values, data=EGR_all, lowess=True, \n",
    "                     scatter_kws={'alpha': 0.5}, \n",
    "                     line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "\n",
    "# Titel and labels\n",
    "plot.set_title('Residuals vs Fitted')\n",
    "plot.set_xlabel('Fitted values')\n",
    "plot.set_ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Assumption  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Independence of Errors Assumption (Durbin-Watson Test)\n",
    "# Durbin-Watson test checks for autocorrelation in residuals (values between 0 and 4, with 2 indicating no autocorrelation)\n",
    "durbin_watson_statistic = sm.stats.stattools.durbin_watson(residuals)\n",
    "print(f\"Durbin-Watson Statistic: {durbin_watson_statistic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normality of residuals using a Q-Q plot\n",
    "\n",
    "sm.qqplot(residuals, line='s')\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Check for constant variance in residuals (Goldfeld-Quandt Test)\n",
    "# This test checks if the variance of residuals is constant across different subsets of the data\n",
    "# It is often used when you suspect heteroscedasticity\n",
    "from statsmodels.stats.diagnostic import het_goldfeldquandt\n",
    "f_stat, p_value, _ = het_goldfeldquandt(residuals, model.model.exog)\n",
    "print(f\"Goldfeld-Quandt Test p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence Diagnostics (Outliers and Influential Points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for outliers (e.g., Cook's distance, leverage)\n",
    "# Identify and investigate potential outliers using Cook's distance or leverage plots\n",
    "\n",
    "#create instance of influence\n",
    "influence = MLR_model.get_influence()\n",
    "\n",
    "#obtain Cook's distance for each observation\n",
    "cooks = influence.cooks_distance\n",
    "\n",
    "plt.scatter(EGR_all.index, cooks[0])\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Cooks Distance')\n",
    "\n",
    "#Adding the index number for each point to investigate further \n",
    "for i, (x, y) in enumerate(zip(EGR_all.index, cooks[0])):\n",
    "    plt.text(x, y, str(i), fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Check for influential points (e.g., DFBETAS, DFFITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the predicted values of our model back to our original df \n",
    "\n",
    "testMLR['predicted'] = MLR_model.predict(testMLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted graph\n",
    "\n",
    "#plt.plot(trainSAR, label='Train') #training data\n",
    "plt.plot(testMLR['transactions'], label='Train') #testing data we want to predict\n",
    "plt.plot(testMLR['predicted'], label='Prediction') #data our model predicted for the taining set \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying how well the model performed \n",
    "\n",
    "boxplote = sns.boxplot(x=testMLR.index.dayofweek, y=testMLR[\"error\"])\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xticks(range(7), [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\n",
    "plt.title(\"Error Distribution by Day of the Week\")\n",
    "\n",
    "# Adding minor ticks on the y-axis\n",
    "plt.grid(axis='y', which='both', linestyle='dotted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confidence Intervals (95% is the default)\n",
    "\n",
    "confidence_intervals = model.conf_int()\n",
    "print(confidence_intervals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Dataset into Training and Test Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into training (prior to 2023) and testing sets (after 2023)\n",
    "\n",
    "train = EGR_dataframe.loc[EGR_dataframe.index < '01-01-2023']\n",
    "test = EGR_dataframe.loc[EGR_dataframe.index >= '01-01-2023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model \n",
    "\n",
    "FEATURES = ['dayofyear', 'dayofweek', 'quarter', 'month', 'year', 'open']\n",
    "TARGET = 'transactions'\n",
    "\n",
    "x_train = train[FEATURES]\n",
    "y_train = train[TARGET]\n",
    "\n",
    "x_test = test[FEATURES]\n",
    "y_test = test[TARGET]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBRegressor Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Grid Search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    # Add more hyperparameters to tune as needed\n",
    "}\n",
    "\n",
    "# Create the XGBoostRegressor model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Initialize the GridSearchCV object with the model, hyperparameter grid, and scoring metric\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, scoring='r2', cv=5)\n",
    "\n",
    "# Fit the GridSearchCV to find the best hyperparameters\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#325 appears to be the sweet spot before overfitting \n",
    "XGBoost_reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree',    \n",
    "                       n_estimators=300, # Number of boosting rounds orig number:325\n",
    "                       early_stopping_rounds=50,\n",
    "                       objective='reg:linear',\n",
    "                       max_depth=5, # Maximum depth of each tree\n",
    "                       learning_rate=0.01) # Learning rate orig number: 0.01\n",
    "\n",
    "#The probelm with the 'oh' error is in the fit code                        \n",
    "XGBoost_reg.fit(x_train, y_train,\n",
    "        eval_set=[(x_train, y_train), (x_test, y_test)],\n",
    "        verbose=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the XGBRegressor Model with Feature Importance Chart, Correlation Matrix, and Residual Mean Square Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance Graph\n",
    "\n",
    "fi = pd.DataFrame(data=XGBoost_reg.feature_importances_,\n",
    "             index=XGBoost_reg.feature_names_in_,\n",
    "             columns=['importance'])\n",
    "fi.sort_values('importance').plot(kind='barh', title='Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = EGR_dataframe.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(np.round(correlation_matrix, decimals=2))\n",
    "\n",
    "# Create correlation matrix plot (heatmap)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating residual mean square error \n",
    "\n",
    "score = np.sqrt(mean_squared_error(test['transactions'], XGBoost_reg.predict(x_test)))\n",
    "print(f'RMSE Score on Test set: {score:0.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the errors between |actual-predicted| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling to get the predicted values for our xgb regressor model into our original 'EGR dataframes' dataset\n",
    "\n",
    "#This is so dumb to avoid copy error BS \n",
    "test = test.copy()\n",
    "\n",
    "# Getting the predicted values from our xgb regressor model as a np array \n",
    "test[\"predicted\"] = np.round(XGBoost_reg.predict(x_test).astype(float), decimals = 2) #need to use np.round because reg.predict(X_test) returns a numpy array and we need to convert \"predicted\" to a float 64 intead of a float 32\n",
    "\n",
    "# Merging the predicted variable from our test dataframe to into the oringal 'EGR dataframes' dataset (we can't just input the 'predicted array directly in this dataset because the oringal dataset has both training and test set)\n",
    "EGR_dataframe = EGR_dataframe.merge(test[[\"predicted\"]], how = \"left\", left_index = True, right_index = True)\n",
    "\n",
    "#Calculating the error between the predicted and actual values\n",
    "EGR_dataframe[\"error\"] = abs(EGR_dataframe[\"transactions\"] - EGR_dataframe[\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting predicted vs actual cirulation data over time\n",
    "\n",
    "ax = EGR_dataframe.loc[(EGR_dataframe.index >= '01-01-2023')]['transactions'] \\\n",
    "    .plot(figsize=(15, 5), title='Week Of Data')\n",
    "\n",
    "EGR_dataframe.loc[(EGR_dataframe.index > '01-01-2023')]['predicted'] \\\n",
    "    .plot()\n",
    "\n",
    "plt.legend(['True Data','Prediction'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the error variable by day of the week and seeing if we have trouble predicting particular days \n",
    "\n",
    "boxplote = sns.boxplot(x=EGR_dataframe.index.dayofweek, y=EGR_dataframe[\"error\"])\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xticks(range(7), [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\n",
    "plt.title(\"Error Distribution by Day of the Week\")\n",
    "\n",
    "# Adding minor ticks on the y-axis\n",
    "plt.grid(axis='y', which='both', linestyle='dotted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this example, I've created two grouped_data variables: grouped_data_mean and grouped_data_median. Each variable represents the mean and median error values grouped by the day of the week.\n",
    "#The bar chart is created using plt.bar(). By specifying different x-coordinates for the mean and median bars (index and index + bar_width), the bars are displayed side by side. The legend is used to differentiate between the mean and median error bars.\n",
    "#To display the values on top of each bar, I've used the annotate() function to add text annotations. The height of each bar is extracted using rect.get_height() and displayed with two decimal places.\n",
    "\n",
    "# Grouping the error variable by the day of the week\n",
    "grouped_data_mean = EGR_dataframe.groupby(EGR_dataframe.index.dayofweek)[\"error\"].mean()\n",
    "grouped_data_median = EGR_dataframe.groupby(EGR_dataframe.index.dayofweek)[\"error\"].median()\n",
    "\n",
    "# Creating a bar chart\n",
    "bar_width = 0.35\n",
    "index = np.arange(7)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(index, grouped_data_mean, bar_width, label='Mean Error')\n",
    "rects2 = ax.bar(index + bar_width, grouped_data_median, bar_width, label='Median Error')\n",
    "\n",
    "# X-axis labels and ticks\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xticks(index + bar_width/2, [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\n",
    "plt.title(\"Mean and Median Error by Day of the Week\")\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the values on top of the bars\n",
    "for rect in rects1+rects2:\n",
    "    height = rect.get_height()\n",
    "    ax.annotate(f'{height:.2f}', xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing all variables from global namespace\n",
    "globals().clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA, SARIMA, and SARIMAX Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for stationarity - need to figure out how to do this - Stationarity allows us to assume past behavior will predict future behavior "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning, checks, and Fixes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data set appropriately into training and testing set\n",
    "\n",
    "trainSAR = EGR_dataframe.loc[EGR_dataframe.index < '01-01-2023']\n",
    "trainSAR = trainSAR[[\"transactions\", \"open\"]] # Keeps just the transactions and date columns in the df (don't need to specify date because it's the index)\n",
    "\n",
    "testSAR = EGR_dataframe.loc[EGR_dataframe.index >= '01-01-2023']\n",
    "testSAR = testSAR[[\"transactions\", \"open\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two ways to check for stationarity: 1.Rolling statistics 2. Augmented Dickey-Fuller test\n",
    "\n",
    "rolling_mean = EGR_dataframe[\"transactions\"].rolling(window=7).mean() \n",
    "rolling_sd =  EGR_dataframe[\"transactions\"].rolling(window=7).std() \n",
    "\n",
    "# Plotting the line graph with modified figure size\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(EGR_dataframe.index, EGR_dataframe['transactions'])\n",
    "plt.plot(rolling_mean, color = 'red', label = 'Rolling Mean')\n",
    "plt.plot(rolling_sd, color = 'black', label = 'Rolling Standard Deviation')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Transactions')\n",
    "plt.title('Line Graph: Transactions')\n",
    "plt.grid(True)\n",
    "\n",
    "# Format the x-axis dates in the format \"Jan '23\"\n",
    "date_format = mdates.DateFormatter(\"%b '%y\")\n",
    "plt.gca().xaxis.set_major_formatter(date_format)\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "\n",
    "# Rotate the date labels for better visibility\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend()\n",
    "\n",
    "# Adding a vertical line where we split the dataset \n",
    "plt.axvline(pd.to_datetime('01-01-2023'), color='black', linestyle='--', label='Vertical Line at x=50')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mean = EGR_dataframe[\"transactions\"].rolling(window=7).mean() \n",
    "rolling_sd =  EGR_dataframe[\"transactions\"].rolling(window=7).std() \n",
    "\n",
    "baskets = np.sqrt(EGR_dataframe[\"transactions\"])\n",
    "\n",
    "plt.plot(baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dickey-Fuller test\n",
    "\n",
    "result = adfuller(EGR_all['transactions'])\n",
    "print('ADF Statistic: {}'.format(result[0]))\n",
    "print('p-value: {}'.format(result[1]))\n",
    "print('Critical Values:')\n",
    "for key, vlaue in result[4].items():\n",
    "    print('\\t{}: {}'.format(key, vlaue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasonal decomposition\n",
    "results = seasonal_decompose(EGR_dataframe[\"transactions\"], period=50) #The period parameter will attempt to identify a repeating pattern or seasonality with a length of 111 data points in the time series.\n",
    "results.plot(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the training and testing data side by side \n",
    "\n",
    "trainSAR['transactions'].plot()\n",
    "testSAR['transactions'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Arima Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "model = auto_arima(trainSAR['transactions'], trace=True, m = 6, suppress_warnings=True, seasonal=True, stepwise=True) # I don't need to specify the transactions variable here since that's the only \n",
    "model.fit(trainSAR['transactions'])                                                                                                                           # but it helps with clarity of code helps with clarity of code other variable in my df                                                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_auto = pd.Series(model.predict(n_periods=len(testSAR))) # with n_periods we predict N steps into the future\n",
    "\n",
    "predictions_auto.index = testSAR.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions for validation set for the auto arima\n",
    "\n",
    "#plt.plot(trainSAR, label='Train') #training data\n",
    "plt.plot(testSAR[\"transactions\"], label='Train') #testing data we want to predict\n",
    "plt.plot(predictions_auto, label='Prediction') #actual predictions \n",
    "plt.axhline(800, color='black', linestyle='--') #data our model predicted for the taining set \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arima Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# ARIMA (AutoRegressive Integrated Moving Average) model with order (p, d, q)\n",
    "\n",
    "# Create and fit the ARIMA model on the training set\n",
    "arima_model = ARIMA(trainSAR['transactions'], order=(4, 0, 0), freq='D')\n",
    "fitted_arima_model = arima_model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_arima = pd.Series(fitted_arima_model.predict(start=len(trainSAR), end=len(trainSAR) + len(testSAR) - 1))\n",
    "\n",
    "predictions_arima.index = testSAR.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing actuals vs predicitions\n",
    "\n",
    "plt.plot(testSAR[\"transactions\"], label='Train')  # Testing data we want to predict\n",
    "plt.plot(predictions_arima, label='Prediction')  # Actual predictions\n",
    "plt.axhline(800, color='black', linestyle='--')  # Data our model predicted for the training set\n",
    "\n",
    "plt.title('ARIMA Model Predictions on Test Set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Example SARIMA (Seasonal ARIMA) model with seasonal_order (P, D, Q, S)\n",
    "\n",
    "sarima_model = SARIMAX(trainSAR['transactions'], order=(2,0,3), seasonal_order=(2,0,1, 6), freq='D')  # Adjust orders and seasonal_order as needed\n",
    "fitted_sarima_model = sarima_model.fit()\n",
    "predictions_sarima = fitted_sarima_model.predict(start=len(trainSAR), end=len(trainSAR) + len(testSAR) - 1)\n",
    "\n",
    "predictions_sarima.index = testSAR.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing actuals vs predicitions\n",
    "\n",
    "plt.plot(testSAR[\"transactions\"], label='Train')  # Testing data we want to predict\n",
    "plt.plot(predictions_sarima, label='Prediction')  # Actual predictions\n",
    "plt.axhline(800, color='black', linestyle='--')  # Data our model predicted for the training set\n",
    "\n",
    "plt.title('SARIMA Model Predictions on Test Set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# SARIMAX - Seasonal ARIMA with exogenous variables - that influence/explain other varaibles in the model\n",
    "\n",
    "sarimax_model = SARIMAX(trainSAR['transactions'], exog=trainSAR[['open']], order=(2,0,3), seasonal_order=(2,0,1, 6), freq='D')\n",
    "fitted_sarimax_model = sarimax_model.fit()\n",
    "predictions_sarimax = fitted_sarimax_model.predict(start=len(trainSAR), end=len(trainSAR) + len(testSAR) - 1, freq='D', exog=testSAR[['open']], dynamic=False)\n",
    "\n",
    "predictions_sarimax.index = testSAR.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing SARIMA with exogenous variables\n",
    "\n",
    "plt.plot(testSAR[\"transactions\"], label='Train')  # Testing data we want to predict\n",
    "plt.plot(predictions_sarimax, label='Prediction')  # Actual predictions\n",
    "plt.axhline(800, color='black', linestyle='--')  # Data our model predicted for the training set\n",
    "\n",
    "plt.title('SARIMAX Model Predictions on Test Set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_log_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def evaluate_forecast(y,pred):\n",
    "    results = pd.DataFrame({'r2_score':r2_score(y, pred),\n",
    "                           }, index=[0])\n",
    "    results['mean_absolute_error'] = mean_absolute_error(y, pred)\n",
    "    results['median_absolute_error'] = median_absolute_error(y, pred)\n",
    "    results['mse'] = mean_squared_error(y, pred)\n",
    "    #results['msle'] = mean_squared_log_error(y, pred)\n",
    "    results['mape'] = mean_absolute_percentage_error(y, pred)\n",
    "    results['rmse'] = np.sqrt(results['mse'])\n",
    "    return results\n",
    "\n",
    "# predictions_sarimax, predictions_sarima, predictions_arima, predictions_auto\n",
    "\n",
    "evaluate_forecast(testSAR['transactions'], predictions_sarimax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ci = pred.conf_int()\n",
    "ax = y['1949':].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Passengers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "if trainSAR.index.name == 'date':\n",
    "    print(\"The 'date' variable is properly saved as the index.\")\n",
    "else:\n",
    "    print(\"The 'date' variable is not the index or not named as 'date'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Could also finsih this assessment off by using this model to acutally predict future values and train the model on the entire dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Long Short-Term Memory (LSTM) Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIGHT BE KINDA COOL TO FEED A MODEL LIKE THIS ALL BRANCHES CHECKOUTS, POTENTIALLY THE MORE DATA WILL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedWV\n",
    "\n",
    "cheese = mergedWV.groupby(\"date\")[\"transactions\"].sum()\n",
    "\n",
    "cheese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reformating the data - each variable is bracketed in a list in case we want to add more variables  \n",
    "\n",
    "# Numpy arrays must all be the same data type, numpy makes it easy to perform mathmatical operations on entire arrays\n",
    "\n",
    "# [[[1], [2], [3], [4], [5]]] [6]   \n",
    "# [[[2], [3], [4], [5], [6]]] [7]\n",
    "# [[[3], [4], [5], [6], [7]]] [8]\n",
    "\n",
    "def df_to_X_y(df, window_size=5): # window size is the last X amount of measurements we're looking at to the predict the next value \n",
    "  df_as_np = df.to_numpy()  # Convertning df to numpy\n",
    "\n",
    "  X = [] # the 5 input numbers as a part of our window size used to predict y\n",
    "  y = [] # the value we predict\n",
    "\n",
    "  for i in range(len(df_as_np)-window_size): \n",
    "    row = [[a] for a in df_as_np[i:i+window_size]] # Creating our row which will store each value within our window size and then wrapping each of those values into a list with the \"[a] for a in\" part\n",
    "    X.append(row) \n",
    "    label = df_as_np[i+window_size]\n",
    "    y.append(label)\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the above functions\n",
    "\n",
    "transactions = EGR_dataframe['transactions']\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "X1, y1 = df_to_X_y(transactions, WINDOW_SIZE)\n",
    "X1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, y_train1 = X1[:550], y1[:550]\n",
    "X_val1, y_val1 = X1[550:600], y1[550:600]\n",
    "X_test1, y_test1 = X1[600:], y1[600:]\n",
    "X_train1.shape, y_train1.shape, X_val1.shape, y_val1.shape, X_test1.shape, y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(InputLayer((5,1)))\n",
    "model1.add(LSTM(64))\n",
    "model1.add(Dense(8, 'relu'))\n",
    "model1.add(Dense(1, 'relu'))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ModelCheckpoint('model1/', save_best_only=True) # only save the best model defined by the one that has the lowest validation loss\n",
    "model1.compile(loss= MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()]) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train1, y_train1, validation_batch_size=(X_val1, y_val1), epochs=10, callbacks=[cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model1 = load_model('model1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model1.predict(X_train1).flatten()\n",
    "train_results = pd.DataFrame(data={'Train Predictions':train_predictions, 'Actuals':y_train1})\n",
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing how well the model did on the training set \n",
    "plt.plot(train_results['Train Predictions'][50:100])\n",
    "plt.plot(train_results['Actuals'][50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = model1.predict(X_val1).flatten()\n",
    "val_results = pd.DataFrame(data={'Val Predictions':val_predictions, 'Actuals':y_val1})\n",
    "val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing how well our model did on our validation data\n",
    "\n",
    "plt.plot(val_results['Val Predictions'][:100])\n",
    "plt.plot(val_results['Actuals'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing how well our model did on our test data (data that has never been before by the model)\n",
    "\n",
    "test_predictions = model1.predict(X_test1).flatten()\n",
    "test_results = pd.DataFrame(data={'Test Predictions':test_predictions, 'Actuals':y_test1})\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_results['Test Predictions'][:100])\n",
    "plt.plot(test_results['Actuals'][:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ideas for creating a forecasting model that didn't pan out \n",
    "\n",
    "#The following models are not really good for time series forecasting \n",
    "\n",
    "#from sklearn import svm\n",
    "#from sklearn.linear_model import Perceptron\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Other idea of trying to map weekdays to a specific number to then input into the model \n",
    "\n",
    "#If we use one-hot encoding like below effectively, we decide that the order of days no longer matters. Is it the case? Not always. Usually, we should not use one-hot encoding to encode days of weeks.\n",
    "    #branch_dfs[branch] = pd.get_dummies(branch_dfs[branch], columns = [\"day\"]) #Converts the categorical variables into dummy variables\n",
    "\n",
    "# Define a dictionary mapping weekdays to numerical values\n",
    "#weekday_mapping = {\n",
    "   # \"Monday\": 1,\n",
    " #   \"Tuesday\": 2,\n",
    " #   \"Wednesday\": 3,\n",
    " #   \"Thursday\": 4,\n",
    " #   \"Friday\": 5,\n",
    "  #  \"Saturday\": 6,\n",
    "  #  \"Sunday\": 7}\n",
    "\n",
    "#branch_dfs[branch].loc[:, \"n_day\"] = branch_dfs[branch][\"day\"].map(weekday_mapping)\n",
    "\n",
    "#angular distance method for input week days into a model (saves the order of the days)\n",
    "# https://www.mikulskibartosz.name/time-in-machine-learning/\n",
    "\n",
    "#circ_day['day_of_week_sin'] = np.sin(circ_day['day'] * (2 * np.pi / 7))\n",
    "#circ_day['day_of_week_cos'] = np.cos(circ_day['day'] * (2 * np.pi / 7))\n",
    "\n",
    "\n",
    "#Splitting the dataset into training and test sets\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(EGR_dataframe.drop('transactions', axis=1), EGR_dataframe['transactions'], test_size=0.2, random_state=42)\n",
    "\n",
    "# The df.drop('target_column', axis=1) selects all columns except the target column as the input features (X)\n",
    "#The test_size parameter specifies the proportion of the data to be allocated to the test set. In the example, 20% of the data is reserved for testing, indicated by test_size=0.2. You can adjust this \n",
    "# value according to your needs.\n",
    "#The random_state parameter sets the seed for the random number generator, ensuring reproducibility of the split. You can change the value of random_state to get different random splits or omit it altogether \n",
    "#to have different splits on each run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
